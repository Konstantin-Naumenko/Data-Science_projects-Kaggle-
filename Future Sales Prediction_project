Задание: необходимо спрогнозировать общий объем продаж для каждого продукта и магазина в следующем месяце.

# sales_train.csv - обучающий набор. Ежедневные исторические данные с января 2013 года по октябрь 2015 года.
# test.csv - тестовый набор. Вам нужно спрогнозировать продажи для этих магазинов и товаров на ноябрь 2015 года.
# sample_submission.csv - образец файла представления в правильном формате.
# items.csv - дополнительная информация о товарах/продуктах.
# item_categories.csv - дополнительная информация о категориях товаров.
# shops.csv - дополнительная информация о магазинах.

# ID - an Id that represents a (Shop, Item) tuple within the test set
# shop_id - unique identifier of a shop
# item_id - unique identifier of a product
# item_category_id - unique identifier of item category
# item_cnt_day - number of products sold. You are predicting a monthly amount of this measure
# item_price - current price of an item
# date - date in format dd/mm/yyyy
# date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33
# item_name - name of item
# shop_name - name of shop
# item_category_name - name of item category

# Вам предоставляются ежедневные исторические данные о продажах. 
# Задача состоит в том, чтобы спрогнозировать общее количество продуктов, 
# проданных в каждом магазине для тестового набора. Обратите внимание, что список 
# магазинов и товаров немного меняется каждый месяц. Создание надежной модели, 
# способной справиться с такими ситуациями, является частью задачи.

# Задача спрогнозировать количество проданных товаров в месяц для каждого магазина
# для этого необходимо сгруппировать данные для каждого магазина по месяцам и просуммировать продажи 


# Импорт библиотек

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns 
%matplotlib inline

# Загрузка данных

#!wget -O https://dl.dropboxusercontent.com/s/7sxnumks3un91oz/sales_train.csv
%cd /content
!gdown --id 1zVWX_ORBPQtF8ctGE-butp0eY7pa7sGW 
!gdown --id 1IYWNnGRudj2QifHkYdn-e7O7omxmUqWQ
!gdown --id 11pTr3d4IikAu_iLVc9Xp4PipXj6dpxgJ
!gdown --id 1nrLF7u8ADAqijdDn9raqHocDM80Wl19_
!gdown --id 1sNaGKBhupSjB-kTHldQcCoEYXO8z7elq
!gdown --id 17ExU6cTIHpgjiw6hTt9jR1aMu0R3Cr0b



sales_train = pd.read_csv('sales_train.csv')
items = pd.read_csv('items.csv')
item_categories = pd.read_csv('item_categories.csv')
shops = pd.read_csv('shops.csv')
test = pd.read_csv('test.csv')


super_set = sales_train.merge(items,left_on='item_id', right_on='item_id')
super_set = super_set.merge(item_categories, left_on='item_category_id', right_on='item_category_id')
super_set = super_set.merge(shops, left_on='shop_id', right_on='shop_id')
# выделяем название города
super_set['city_name'] = super_set.apply( lambda row: row['shop_name'].split(' ')[0] , axis = 1)
super_set['date'] = pd.to_datetime(super_set.date)
# выделяем месяц и год
super_set['year'] = super_set['date'].dt.year
super_set['month'] = super_set['date'].dt.month
super_set['date_before'] = super_set['date'] - pd.DateOffset(months=1)
super_set['year_before'] = super_set['date_before'].dt.year
super_set['month_before'] = super_set['date_before'].dt.month
# super_set['month'] = super_set.apply( lambda row: int(row['date'].split('.')[1]) , axis = 1)
# super_set['year'] = super_set.apply( lambda row: int(row['date'].split('.')[2]) , axis = 1)
super_set

# 
# Добавляем количество продаж в прошлом месяце
# 

month_before = super_set[['date_before','item_id','shop_id']]
month_w_count = super_set[['date','item_id','shop_id','item_cnt_day']]

month_before = month_before.merge( month_w_count, left_on=['date_before','item_id','shop_id'], right_on=['date','item_id','shop_id'] )
month_before = month_before.rename(columns={"item_cnt_day": "item_cnt_before_month"})
month_before = month_before.drop('date',1)
# month_before
super_set = super_set.merge( month_before, left_on=['date_before','shop_id','item_id'], right_on=['date_before','shop_id','item_id'] )
# month_before.isnull().values.any()


# Проверка данных и первичная обработка

super_set.describe()
#print(super_set.isnull().sum())

# сравним количество уникальных элементов по обучающей и тестовой выборкам
print(super_set.nunique())
print(45*"*")
print(test.nunique())

# проверим вхождение элементов в ту и другую выборки
missing_shops = [s for s in test["shop_id"].unique() if s not in super_set["shop_id"].unique()]
missing_items = [i for i in test["item_id"].unique() if i not in super_set["item_id"].unique()]
missing_shops_test = [s for s in super_set["shop_id"].unique() if s not in test["shop_id"].unique()]
missing_items_test = [i for i in super_set["item_id"].unique() if i not in test["item_id"].unique()]
print(45*"*")
print(f"Number of missing shops in train_df:", len(missing_shops))
print(f"Number of missing items in train_df:", len(missing_items))
print(f"Number of missing shops in test_df:", len(missing_shops_test))
print(f"Number of missing items in test_df:", len(missing_items_test))
print(45*"*")

# удаляем повторяющиеся данные
print('Number of duplicates:', len(super_set[super_set.duplicated()]))
super_set = super_set.drop_duplicates()
super_set.shape

# удаляем строки с отрицательными значениями по признаку "item_price" и по классу "item_cnt_day",
# предполагая, что стоимость и продажи не могут быть отрицательными 
print(super_set[super_set['item_price'] < 0 ]['item_price'])
print(super_set[super_set['item_cnt_day'] < 0 ]['item_cnt_day'])
super_set = super_set[super_set['item_price'] > 0]
super_set = super_set[super_set['item_cnt_day'] > 0]
super_set = super_set[super_set['item_cnt_before_month'] > 0]
super_set.shape

# применяем перцентиль к выбросам по 'item_cnt_day', 'item_price'
columns = ['item_cnt_day','item_price']
for col in columns:
    percentile = super_set[col].quantile(0.99)
    print(col, percentile)
    super_set.loc[super_set[col] > percentile, col] = percentile
    super_set[col] = np.where(super_set[col] > percentile, percentile, super_set[col])
super_set.shape    

# узнаем кол-во уникальных элементов по классу 
super_set['item_cnt_day'].value_counts()

# узнаем кол-во уникальных элементов по признакам  
print('Unique_date:', len(super_set['date'].unique()))
print('Unique_date_block_num:', len(super_set['date_block_num'].unique()))
print('Unique_shop_id:', len(super_set['shop_id'].unique()))
print('Unique_item_id:', len(super_set['item_id'].unique()))
print('Unique_item_price:', len(super_set['item_price'].unique()))
print('Unique_city_name:', len(super_set['city_name'].unique()))
# небольшая аналитика по классам
print( "item_cnt_day mean: {}, std_dev: {}, min: {}, max: {}, median: {}".format( super_set['item_cnt_day'].mean(), super_set['item_cnt_day'].std(), super_set['item_cnt_day'].min(), super_set['item_cnt_day'].max(), super_set['item_cnt_day'].median() ) )

# узнаем количество наименований уникальных товаров по каждой товарной категории 
gr_categories=items.groupby(['item_category_id']).size().sort_values(ascending=False)
# gr_categories.sort
print(gr_categories)
gr_categories.plot(kind='pie', subplots=True, figsize=(8, 8))
plt.title("Распределение количества наименований уникальных товаров по категориям")
plt.ylabel("")
plt.show()
print('Пять наиболее крупных категорий товаров:', gr_categories.nlargest(5))

# группируем данные по месяцам и магазинам - получаем суммарное количество продаж в месяц для каждоо товара в магазине
agg_super_set = super_set.groupby(by=['month','year','shop_id', 'item_id', 'item_category_id','month_before','year_before']).agg({'item_price':'mean','item_cnt_day':'sum'}).reset_index()
agg_before = super_set.groupby(by=['month_before','year_before','shop_id', 'item_id' ]).agg({'item_cnt_before_month':'sum'}).reset_index()
agg_super_set = agg_super_set.merge( agg_before, left_on=['month_before','year_before','shop_id', 'item_id'], right_on=['month_before','year_before','shop_id', 'item_id'] )
agg_super_set

# добавляем колонки:
# количество проданных товаров магазином в этот месяц
ss = super_set.groupby(by=['month','year','shop_id', ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_month"})
agg_super_set = agg_super_set.merge( ss, left_on=['month','year','shop_id'], right_on=['month','year','shop_id'] )

# количество проданных товаров в этом магазине за год
# ss = super_set.groupby(by=['year','shop_id','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_item_year"})
# agg_super_set = agg_super_set.merge( ss, left_on=['year','shop_id','item_id'], right_on=['year','shop_id','item_id'] )

# количество проданных товаров в этом году
ss = super_set.groupby(by=['year','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_item_year"})
agg_super_set = agg_super_set.merge( ss, left_on=['year','item_id'], right_on=['year','item_id'] )

# количество проданных товаров в прошлом месяце вообще
ss = super_set.groupby(by=['month_before','year','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_item_month"})
agg_super_set = agg_super_set.merge( ss, left_on=['month_before','year','item_id'], right_on=['month_before','year','item_id'] )

# количество проданных товаров категории в этом магазине за прошлом месяц
ss = super_set.groupby(by=['month_before','year','shop_id','item_category_id' ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_month_cat"})
agg_super_set = agg_super_set.merge( ss, left_on=['month_before','year','shop_id','item_category_id'], right_on=['month_before','year','shop_id','item_category_id'] )

# количество проданных товаров категории в этом магазине за год
ss = super_set.groupby(by=['year','shop_id','item_category_id' ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_year_cat"})
agg_super_set = agg_super_set.merge( ss, left_on=['year','shop_id','item_category_id'], right_on=['year','shop_id','item_category_id'] )

# количество товаров проданных магазином 
ss = super_set.groupby(by=['shop_id','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_item"})
agg_super_set = agg_super_set.merge( ss, left_on=['shop_id','item_id'], right_on=['shop_id','item_id'] )

# количество товаров в магазине
ss = super_set.groupby(by=['shop_id' ], as_index = False).agg({'item_id':'count'})
ss = ss.rename(columns={"item_id": "item_shop_count"})
agg_super_set = agg_super_set.merge( ss, left_on=['shop_id'], right_on=['shop_id'] )

# количество товаров по категориям в магазине
ss = super_set.groupby(by=['shop_id','item_category_id' ], as_index = False).agg({'item_id':'count'})
ss = ss.rename(columns={"item_id": "item_shop_cat_count"})
agg_super_set = agg_super_set.merge( ss, left_on=['shop_id','item_category_id'], right_on=['shop_id','item_category_id'] )

# продажи определенного товара (выручка)
super_set['Sales_per_item'] = super_set['item_cnt_day'] * super_set['item_price']
agg_super_set['Sales_per_item'] = super_set['Sales_per_item']

# вернул колонку "date_block_num"
agg_super_set['date_block_num'] = super_set['date_block_num']

agg_super_set

# 
# =============================================================================
#                  Тестовые данные
# =============================================================================
# 

test = pd.read_csv('test.csv')
# нябрь 2015 года
test['month'] = 10 
test['year'] = 2015
# добалвяем цену и категорию
ss = super_set.groupby(by=['item_id','shop_id','item_category_id' ], as_index = False).agg({'item_price':'mean'})
test = test.merge( ss, left_on=['item_id','shop_id'], right_on=['item_id','shop_id'] )

# добавляем колонки:
# количество проданных товаров магазином в этот месяц
# ss = super_set.groupby(by=['month','year','shop_id', ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_month"})
# test = test.merge( ss, left_on=['month','year','shop_id'], right_on=['month','year','shop_id'] )

# # количество проданных товаров в этом магазине за год
# ss = super_set.groupby(by=['year','shop_id','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_item_year"})
# test = test.merge( ss, left_on=['year','shop_id','item_id'], right_on=['year','shop_id','item_id'] )

# # количество проданных товаров в этом году
# ss = super_set.groupby(by=['year','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_item_year"})
# test = test.merge( ss, left_on=['year','item_id'], right_on=['year','item_id'] )

# # количество проданных товаров в этом месяце вообще
# ss = super_set.groupby(by=['month','year','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_item_month"})
# test = test.merge( ss, left_on=['month','year','item_id'], right_on=['month','year','item_id'] )

# # количество проданных товаров категории в этом магазине за месяц
# ss = super_set.groupby(by=['month','year','shop_id','item_category_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_month_cat"})
# test = test.merge( ss, left_on=['month','year','shop_id','item_category_id'], right_on=['month','year','shop_id','item_category_id'] )

# # количество проданных товаров категории в этом магазине за год
# ss = super_set.groupby(by=['year','shop_id','item_category_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_year_cat"})
# test = test.merge( ss, left_on=['year','shop_id','item_category_id'], right_on=['year','shop_id','item_category_id'] )

# # количество товаров проданных магазином 
# ss = super_set.groupby(by=['shop_id','item_id' ], as_index = False).agg({'item_cnt_day':'sum'})
# ss = ss.rename(columns={"item_cnt_day": "item_cnt_shop_item"})
# test = test.merge( ss, left_on=['shop_id','item_id'], right_on=['shop_id','item_id'] )

# # количество товаров в магазине
# ss = super_set.groupby(by=['shop_id' ], as_index = False).agg({'item_id':'count'})
# ss = ss.rename(columns={"item_id": "item_shop_count"})
# test = test.merge( ss, left_on=['shop_id'], right_on=['shop_id'] )

# # количество товаров по категориям в магазине
# ss = super_set.groupby(by=['shop_id','item_category_id' ], as_index = False).agg({'item_id':'count'})
# ss = ss.rename(columns={"item_id": "item_shop_cat_count"})
# test = test.merge( ss, left_on=['shop_id','item_category_id'], right_on=['shop_id','item_category_id'] )

# # продажи определенного товара (выручка)
# super_set['Sales_per_item'] = super_set['item_cnt_day'] * super_set['item_price']
# test['Sales_per_item'] = super_set['Sales_per_item']

# # вернул колонку "date_block_num"
# test['date_block_num'] = super_set['date_block_num']

# Динамика продаж  
agg_sales_date = super_set.groupby(by=['year','month'])['item_cnt_day'].sum()
agg_sales_date.head()
agg_sales_date.describe
plt.minorticks_on()
plt.grid(which='major')
plt.grid(which='minor', linestyle=':')
agg_sales_date.plot(figsize=(12,6))

#Десять самых продаваемых товаров
agg_items = super_set.groupby(by=['item_id'],as_index = False).agg({'item_cnt_day':'sum'}).sort_values(by=['item_cnt_day'],
                                                                                     ascending=False)
agg_items = agg_items.merge( items, on=['item_id'])

agg_items.head(10)

# Количество продаж по магазинам 
agg_sales = super_set.groupby(by=['shop_id'])['item_cnt_day'].sum().reset_index().sort_values(by=['item_cnt_day'],
                                                                                     ascending=False)
fig, ax = plt.subplots(figsize=(20,5))
sns.barplot(x='shop_id', y='item_cnt_day', data=agg_sales,order=agg_sales.index)
ax.set_title('Sales of Shops')
plt.show()
# agg_sales.index[0]

# Количество товаров по магазинам 
agg_items = agg_super_set.groupby(by=['shop_id'])['item_shop_count'].sum().reset_index().sort_values(by=['item_shop_count'],
                                                                                     ascending=False)
fig, ax = plt.subplots(figsize=(20,5))
sns.barplot(x = 'shop_id', y = 'item_shop_count', data = agg_items, order = agg_items.index)
ax.set_title('Items for Shops')
plt.show()

# Проверка корреляции кол-ва видов товаров и продаж
agg_items = super_set.groupby(by=['shop_id'])['item_id'].unique().apply(lambda x: len(x)).reset_index().rename(columns=
                                {"item_id":'Total Items'}).reset_index().sort_values(by=['Total Items'],
                                                                                      ascending=False)
fig, ax1 = plt.subplots(figsize=(20,6))
color = 'tab:green'
#bar plot creation
ax1.set_title('Total Items Vs Sales of Shop', fontsize=16)
ax1.set_xlabel('shop_id', fontsize=16)
ax1.set_ylabel('Sales', fontsize=16)
ax1 = sns.lineplot(x='shop_id', y='item_cnt_day', data=agg_sales, palette='summer')
ax1.tick_params(axis='y')
plt.legend(['Sales'])
#specify we want to share the same x-axis
ax2 = ax1.twinx()
color = 'tab:red'
#line plot creation
ax2.set_ylabel('Total Items', fontsize=16)
ax2 = sns.lineplot(x='shop_id', y='Total Items', data = agg_items, color=color)
ax2.tick_params(axis='y', color=color)
#show plot
plt.legend(['Total Items'])
plt.show()

# Количество продаж по месяцам по магазинам
fig, axs = plt.subplots(6, 2, figsize=(20,30) )

r=0
c=0
for m in range(1,13):
  i = m-1
  # m = '%02d' % m
  if c==1:
    r+=1
    c=0
  if ( i%2 ):
    c=1
  
  m_data = agg_super_set[ agg_super_set['month']==m ]
  sns.barplot(ax=axs[ r, c ], x='shop_id', y='item_cnt_day', data=m_data)
  axs[ r, c ].set_title('Sales of Shops by month %d' % m )

plt.show()

# продажи магазинов по месяцам
agg_super_set2 = agg_super_set[['date_block_num','shop_id','item_cnt_day']]
dt = pd.pivot_table(index = 'date_block_num', data = agg_super_set, columns='shop_id', aggfunc='sum')
dt = dt.item_cnt_day
dt.columns.name = 'No. of Shops'
dt.index.name='No. of Months'
dt.fillna(0, inplace=True)
dt

# смотрим на количество продаж товаров по месяцам/годам
fig,axes = plt.subplots(1,1,figsize=(24,9))
sales_data_tmp = agg_super_set[['year','month','item_cnt_day']].pivot_table(index=['month'],columns=['year'],aggfunc={"item_cnt_day":np.sum})
axes.plot(sales_data_tmp)
axes.set_title('Total number of units sold')
axes.legend(labels=[i[1] for i in sales_data_tmp.columns])
plt.suptitle('Monthly Sales',fontsize="28")
plt.show()

# смотрим на суммы продаж товаров по месяцам/годам
fig,axes = plt.subplots(1,1,figsize=(24,9))
sales_data_tmp = agg_super_set[['year','month','Sales_per_item']].pivot_table(index=['month'],columns=['year'],aggfunc={"Sales_per_item":np.sum})
axes.plot(sales_data_tmp)
axes.set_title('Total sales per month')
axes.legend(labels=[i[1] for i in sales_data_tmp.columns])
plt.suptitle('Monthly Sales',fontsize="28")
plt.show()

# смотрим на изменение цен по годам
fig,axes = plt.subplots(1,1,figsize=(24,9))
price_data_tmp = agg_super_set[['year','month','item_price']].pivot_table(index=['month'],columns=['year'],aggfunc={"item_price":np.sum})
axes.plot(price_data_tmp)
axes.set_title('Prices per months')
axes.legend(labels=[i[1] for i in price_data_tmp.columns])
plt.suptitle('Monthly Prices',fontsize="28")
plt.show()

# agg_super_set['item_cnt_day'] = agg_super_set['item_cnt_day'].quantile(0.99)
sns.histplot( agg_super_set['item_cnt_day'] ,  binwidth=1 )
agg_super_set['item_cnt_day'].describe()

# Сортируем по времени, сбрасываем индекс
agg_super_set = agg_super_set.sort_values(by=['month','year'], ascending=True).reset_index(drop=True)

# Матрица корреляций между параметрами
f, ax = plt.subplots(figsize=(15, 12))
ax = sns.heatmap(agg_super_set.corr(),  cmap="YlGnBu", annot=True)

# оставляем датасет без колонки ответов
X = agg_super_set.drop(['item_cnt_day'], axis=1)
y = agg_super_set['item_cnt_day']

# Нормирование данных

# нормализуем данные и сохраняем нормализаторы

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
scalers = {}
for _ in agg_super_set.columns:
  if _ in ['month','shop_id','item_id','item_category_id','item_cnt_day']:
    scalers[_] = LabelEncoder()
    scalers[_].fit( agg_super_set[_].values )
  else:
    scalers[_] = MinMaxScaler()
    scalers[_].fit( agg_super_set[_].values.reshape(-1, 1) )
  # print(agg_super_set[_].values)
  
scalers

# Это важно - все пустые значения заполняем промежуточными
# если будут NaN то модель не обучится
agg_super_set.fillna(method='pad', inplace=True)

# 
# Проводим нормализацию датасета, в зависмости от типа данных
# 
# Предикторы
for _ in X.columns:
  if type(scalers[_])==LabelEncoder:
    X[_] = scalers[_].transform( agg_super_set[_].values )
  else:
    X[_] = scalers[_].transform( agg_super_set[_].values.reshape(-1,1) )

# Предиктаты
_ = 'item_cnt_day'
if type(scalers[_])==LabelEncoder:
  y_ = scalers[_].transform( agg_super_set[_].values )
else:
  y_ = scalers[_].transform( agg_super_set[_].values.reshape(-1,1) )

# Если y будем считать по регрессии, то можно оставить как числовое значение
y=y_.reshape(-1,1)
# если надо перевести y в категориальный признак, тогда каждое значение предиктата будет массивом 
# с размерностью len(scalers[_].classes_)

# arr_len = len(scalers[_].classes_)
# y=[]
# for _ in y_:
#   item = [0]*arr_len
#   item[_] = 1
#   y.append(item)


# Разбивка на обучение и контроль

# разбиваем индексы на две части 80/20
from sklearn.model_selection import train_test_split


 # Обучение модели
 

# 
# Обучение лучше всего запускать на GPU
# 
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.layers import LSTM, Embedding, Conv1D, MaxPooling1D, Flatten
from keras.layers import BatchNormalization
from tensorflow.keras.optimizers import SGD
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, explained_variance_score
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))


## RandomForestRegressor

#%%script false --no-raise-error
# размер массива категорий
_ = 'item_cnt_day'
arr_len = len(scalers[_].classes_)
y=[]
for _ in y_:
  item = [0]*arr_len
  item[_] = 1
  y.append(item)


train_X, test_X, train_Y, test_Y = train_test_split(X,y,train_size=0.4, test_size=0.3)
train_X = np.array(train_X)
test_X = np.array(test_X)
train_Y = np.array(train_Y)
test_Y = np.array(test_Y)

model = RandomForestRegressor()
model.fit(train_X, train_Y)
# 02. Predict # 
predictions = model.predict(test_X)
# 03 Evaluate # 
print('--------------------*---------------------------')
print('Model: ', str(model).split('_')[-1])
print('Mean Absolute Error: ',mean_absolute_error(test_Y, predictions))
print('Explained Variance: ',explained_variance_score(test_Y, predictions))

#%%script false --no-raise-error
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import roc_curve
# надо найти категорию, значение в которой максимально. Позиция этой категории и будет ответом
pred_values = []
y_values = []

for _ in predictions:
  pred_values.append( np.argmax(_)+1 )
for _ in test_Y:
  y_values.append( np.argmax(_)+1 )
rmse = mean_squared_error(y_values, pred_values, squared=False)
mse = mean_squared_error(y_values, pred_values)
print( "RMSE: %2.2f" % rmse )
print( "mse: %2.2f" % mse )
print('Mean Absolute Error: %2.2f' % mean_absolute_error(y_values, pred_values))
print('Explained Variance: %2.2f' % explained_variance_score(y_values, pred_values))

#%%script false --no-raise-error
model.feature_importances_
feats = {}
for feature, importance in zip(X.columns, model.feature_importances_):
  feats[feature] = importance

importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Importance'})
importances = importances.sort_values(by='Importance', ascending=False)
importances = importances.reset_index()
importances = importances.rename(columns={'index': 'Features'})
# print(importances)
sns.barplot(x=importances['Importance'], y=importances['Features'], data=importances, color='skyblue')

## Keras learn

Keras LSTM

_ = 'item_cnt_day'
arr_len = len(scalers[_].classes_)
y=[]
for _ in y_:
  item = [0]*arr_len
  item[_] = 1
  y.append(item)

# y=y_.reshape(-1,1)

train_X, test_X, train_Y, test_Y = train_test_split(X,y,test_size=0.3, shuffle=True)
train_X = np.array(train_X)
test_X = np.array(test_X)
train_Y = np.array(train_Y)
test_Y = np.array(test_Y)


columns_count = train_X.shape[1]
batch_size = 200
# обрезаем массивы по батчу
array_w_batch_size = train_X.shape[0]//batch_size*batch_size
train_X = train_X[:array_w_batch_size]
train_Y = train_Y[:array_w_batch_size]
array_w_batch_size = test_X.shape[0]//batch_size*batch_size
test_X = test_X[:array_w_batch_size]
test_Y = test_Y[:array_w_batch_size]
print('train_X',train_X.shape)
print('train_Y',train_Y.shape)
print('test_X',test_X.shape)
print('test_Y',test_Y.shape)
print(train_Y[:10])
def get_model():
  model = Sequential()
  # model.add(Embedding(875,3))
  model.add(LSTM( 128 , batch_input_shape=(batch_size, columns_count, 1), return_sequences=True,))
  model.add(LSTM( 128 , return_sequences=True ))
  model.add(LSTM( 128 ,  ))
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*10, activation='selu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*4, activation='selu'))
  model.add(Dense(columns_count*3, activation='selu'))
  model.add(Dense(columns_count*3, activation='selu'))
  model.add(Dense(columns_count*3, activation='selu'))
  model.add(Dense(columns_count*2, activation='selu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*2, activation='relu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.2))
  model.add(Dense(arr_len, activation='softmax'))
  opt = SGD(learning_rate=0.01, momentum=0.9)
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  # plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)


model = get_model()
model.summary()
# fit the keras model on the dataset
hist = model.fit(train_X, train_Y, epochs=40, batch_size=batch_size, verbose=1, validation_data=(test_X, test_Y))
# evaluate the keras model
# _, accuracy = model.evaluate(test_X, test_Y)
# print('Accuracy: %.2f' % (accuracy*100))



# print(test_Y[:32])
# print(model.predict(test_X[:32]))
# print(test_Y[3:4])
# print(model.predict(test_X[3:4]))
plot_loss(hist)

from sklearn.metrics import mean_squared_error

batch_size = 32
# обрезаем массивы по батчу
array_w_batch_size = test_X.shape[0]//batch_size*batch_size
test_X = test_X[:array_w_batch_size]
test_Y = test_Y[:array_w_batch_size]
print('train_X',train_X.shape)
print('train_Y',train_Y.shape)
print('test_X',test_X.shape)
print('test_Y',test_Y.shape)

pred_Y = model.predict(test_X)
# надо найти категорию, значение в которой максимально. Позиция этой категории и будет ответом
pred_values = []
y_values = []
for _ in pred_Y:
  pred_values.append( np.argmax(_)+1 )
for _ in test_Y:
  y_values.append( np.argmax(_)+1 )
rmse = mean_squared_error(y_values, pred_values, squared=False)
mse = mean_squared_error(y_values, pred_values)
print( "RMSE: %2.2f" % rmse )
print( "mse: %2.2f" % mse )
print('Mean Absolute Error: %2.2f' % mean_absolute_error(y_values, pred_values))
print('Explained Variance: %2.2f' % explained_variance_score(y_values, pred_values))

### Keras **CONV**

_ = 'item_cnt_day'
arr_len = len(scalers[_].classes_)
y=[]
for _ in y_:
  item = [0]*arr_len
  item[_] = 1
  y.append(item)

# y=y_.reshape(-1,1)

train_X, test_X, train_Y, test_Y = train_test_split(X,y,test_size=0.3, shuffle=True)
train_X = np.array(train_X)
test_X = np.array(test_X)
train_Y = np.array(train_Y)
test_Y = np.array(test_Y)


columns_count = train_X.shape[1]
# меньше батчи - лучше обучается
batch_size = 200
# обрезаем массивы по батчу
array_w_batch_size = train_X.shape[0]//batch_size*batch_size
train_X = train_X[:array_w_batch_size]
train_Y = train_Y[:array_w_batch_size]
array_w_batch_size = test_X.shape[0]//batch_size*batch_size
test_X = test_X[:array_w_batch_size]
test_Y = test_Y[:array_w_batch_size]
print('train_X',train_X.shape)
print('train_Y',train_Y.shape)
print('test_X',test_X.shape)
print('test_Y',test_Y.shape)
print(train_Y[:10])
def get_model():
  model = Sequential()
  # model.add(Embedding(875,3))
  model.add(Conv1D(128, 3, batch_input_shape=(batch_size, columns_count, 1), activation="relu"))
  model.add(MaxPooling1D(pool_size=3, strides=3))
  model.add(Conv1D(128, 3, batch_input_shape=(batch_size, columns_count, 1), activation="selu"))
  model.add(MaxPooling1D(pool_size=2, strides=2))
  model.add(Flatten())
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*10, activation='selu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*8, activation='selu'))
  model.add(Dense(columns_count*5, activation='selu'))
  model.add(Dense(columns_count*5, activation='selu'))
  model.add(Dense(columns_count*5, activation='selu'))
  model.add(Dense(columns_count*4, activation='selu'))
  model.add(BatchNormalization())
  model.add(Dropout(0.1))
  model.add(Dense(columns_count*2, activation='sigmoid'))
  model.add(BatchNormalization())
  model.add(Dropout(0.2))
  model.add(Dense(arr_len, activation='softmax'))
  opt = SGD(learning_rate=0.01, momentum=0.9)
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

def plot_loss(history):
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  # plt.ylim([0, 10])
  plt.xlabel('Epoch')
  plt.ylabel('Error [MPG]')
  plt.legend()
  plt.grid(True)


model = get_model()
model.summary()
# fit the keras model on the dataset
hist = model.fit(train_X, train_Y, epochs=130, batch_size=batch_size, verbose=1, validation_data=(test_X, test_Y))
# evaluate the keras model
# _, accuracy = model.evaluate(test_X, test_Y)
# print('Accuracy: %.2f' % (accuracy*100))



print(test_Y[:10])
print(model.predict(test_X[:10]))
# print(test_Y[3:4])
# print(model.predict(test_X[3:4]))
plot_loss(hist)

from sklearn.metrics import mean_squared_error
pred_Y = model.predict(test_X)
# надо найти категорию, значение в которой максимально. Позиция этой категории и будет ответом
pred_values = []
y_values = []
for _ in pred_Y:
  pred_values.append( np.argmax(_)+1 )
for _ in test_Y:
  y_values.append( np.argmax(_)+1 )
rmse = mean_squared_error(y_values, pred_values, squared=False)
mse = mean_squared_error(y_values, pred_values)
print( "RMSE: %2.2f" % rmse )
print( "mse: %2.2f" % mse )
print('Mean Absolute Error: %2.2f' % mean_absolute_error(y_values, pred_values))
print('Explained Variance: %2.2f' % explained_variance_score(y_values, pred_values))
